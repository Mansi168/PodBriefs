{
    "podcast_details": {
        "podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)",
        "episode_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643",
        "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress",
        "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI Podcast. I'm your host, Sam Charrington, and today I'm joined by Gokul Swamy. Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University. Wherever you're listening to today's show, make sure you're subscribed and be sure to like, rate, and review the show. Gokul, welcome to the podcast. Yeah, thank you so much for having me. Hey, I'm looking forward to digging into our conversation. We'll be talking about your research into the fields of efficient interactive learning and making good decisions without observable boundaries with a particular emphasis on a few of the papers that you are presenting at this year's ICML conference. Before we dig into that though, I'd love to have you share a little bit about your background and how you came to the field. Yeah, good question. So I just wrapped up my third year at CMU. I spent a bunch of time there working on different things that are about sort of algorithms for this field called imitation learning, which is, broadly speaking, about how you can try and learn to make good decisions from data. Before that, I spent a few years at Berkeley, where I was a master's and undergrad student, and there I was working on methods for human-robot interaction. Even before that, I grew up in San Diego, where I tried to just maximize the amount of time I spent on the beach. Sounds worthwhile. Tell us a little bit about your research interests and the focus of your work. Yeah. So I think over the last few years, what I've been really trying to think about is how we can try and learn to make sequence of decisions well from observing data. some sort of expert demonstrated during the same thing. So perhaps the most kind of intuitive example of this is something like self-driving cars. So, you know, we put some person in a car, we strap the car up with sensors, we get them to drive all around Pittsburgh, and then we want to learn a program, let's the car do the same thing. And I spent a lot of time thinking about what are the right sorts of algorithms for that problem. And I think there's two parts of that that I'm most interested in. The first is how we do this efficiently. And I mean efficiently in various senses, both in terms of the amount of data you need to use, the amount of compute you need to use, stuff like that. The other thing I'm really interested in is, if we're trying to get a car to do the same thing as a person, they don't have the same sort of observation space in some sense, right? They don't see exactly the same way. They don't have the exact same sensors. And at that point, there's all these sorts of things that show up that affect a relationship you care about that you don't really observe. So let's say you're trying to predict from some variable x to some variable y, and there's some unobserved variable u that affects both. This is usually called an unobserved confounder. And you know, here I'm trying to predict from, you know, some sort of observations, some sort of how much I want to the steering wheel, how much I want to press the gas pedal, stuff like that. For example, your self-driving car might not be able to pick up on the fact that a hand gesture from somebody in the car across from them actually means something. And when you kind of have this sort of partial observability, I think it's a really interesting question of how do you still learn well? So I've been thinking about both in the sort of idealized setting, how do you do it efficiently, and in the sort of more real world setting where you don't perhaps get access to the same pieces of information, how do you make decisions well there? Awesome. And how does that tie into the research that you're presenting at the conference this year? Yeah, so I think we have a few papers at the conference, which I'm very thankful for. And they sort of touch on different parts of those topics. So we have one paper at the main conference, which is really focused on the question of how do we do imitation learning efficiently. So that paper, I think, is really cool because it's a paper where the theory is very elegant. It's not a very complicated idea mathematically. But it really, really works in practice. So we're quite excited about that. Is this the inverse RL paper? Yeah, yeah. So I can maybe talk a little bit about that if that would be good. Yeah, let's dig into it. Sure. So just before you do the the title of the paper is inverse reinforcement learning without reinforcement learning. which is an intriguing title. Thank you. Broadly speaking, in scientists, we have forward problems and inverse problems. The forward problem is usually, okay, I have some objective function and I'm going to optimize it to get something. The inverse problem is, okay, given behavior of the optimal thing, what was the thing I was trying to optimize in the first place? And in the sort of inverse reinforcement learning setting, the forward problem is reinforcement learning. So I give you a word function. I want you to find the optimal policy under this word function. The inverse problem is, OK, I gave you data from the optimal policy. I want you to figure out what was the word function that was being optimized here. So if you think about the driving example, right? I might want to try to extract a function that tells you, okay, how much does this person care about, you know, staying away from the cars that are in front of them? How much do they care about, you know, adhering to the speed limits, things like that? And I think it's a very natural question to ask, like, well, can't I just write down a function that does that? And it's actually really quite hard when you try to do it, right? Because I can tell you that, okay, I definitely care about observing the speed limit. I care about staying away from people that are in front of me. But exactly how much I care about those two things relative to each other is very hard to write down. So I think it makes a lot of sense to try and learn from data. Yeah. Yeah. It sounds difficult enough to do manually when you're only thinking about one of those relationships. But when you're talking about highly dimensional set of features, it sounds very, very different. For sure, yeah, yeah. And especially if you really want to have good human-like behavior in a variety of settings, right? Most people are just paying attention to one thing when they're driving, right? There's all these things are balancing in their head. So you really do need to try and actually learn from data how to do this. And so talk about the motivation behind trying to apply inverse RL without RL. Yeah, so. And maybe was that a motivation or was that a result? So I think the motivation here is really like how we can make inverse reinforcement learning more computationally efficient. So there's sort of like pros and cons, I think, to the sort of inverse reinforcement learning approach to things. I think the pro is basically that you don't suffer from something called compounding errors. So the simplest approach you could think of trying to do for imitation learning, right, is basically purely offline supervised learning approach. So what I do is I got a set of expert states, I got a set of expert actions, I just regress between them, and I just roll out my car. The issue is that Of course, at some point, you're going to make a mistake and you're going to end up in a situation in which you didn't have any data before. And then you're not going to know what to do. And you're just going to keep making mistakes over and over again. So if you kind of look at the early self-driving work in the late eighties, this is sort of what they were trying to do. And, you know, the cars drove a little bit, but as soon as you try to do something hard with them, they didn't work. The benefit of the inverse reinforcement learning approaches is because you're actually rolling out the learner's policy, seeing how things go, you're able to see, OK, when I turn left, this is actually where I ended up. You're not just looking at states from the expert state distribution. So if you kind of think of it as this sort of set of sort of maybe in the kind of distribution shift setting, basically what interaction gives us is it lets us kind of get samples on the test distribution because we can actually drive and see where we end up. So this is really nice because it lets you, you know, kind of see where you're going to end up, so you're not going to make mistakes you don't expect. But the challenge, of course, then, is that you have to repeatedly interact with the simulator over and over again. And you have to solve these hard reinforcement learning problems. So conceptually, the way inverse reinforcement learning works is basically it's almost like a GAN, but in the space of trajectories. So your generator here is like a policy that's kind of coupled with world models or dynamics kind of give you trajectories. And your discriminator here basically is saying, OK, I want to look at the difference between expert and learner trajectories. And then you do a policy update that is, OK, let me take this learned discriminator and use it as a word function and do reinforcement learning with that. So the issue then is that you need to repeatedly solve a hard reinforcement learning problem at every single step of this procedure. So that means you're going to spend a huge number of samples, right? We already know that reinforcement learning is really, really hard to actually do in the real world on problems, right? And if I'm asking you to do it over and over again, well, it's kind of difficult. So our question in this work was, OK, there's all these benefits to the inverse reinforcement learning approach, but there are these severe computational issues. So how can we try and actually speed this up quite a bit? And you arrived at an approach that does not use reinforcement learning, in fact? Yeah, so it's effectively an approach that allows us to like learning to make sequential decisions, but without the sort of part that makes reinforcement learning hard, which is exploration, right? So maybe sort of a concrete example I like here is something like, imagine what we want to do is sort of act optimally in some sort of problem that looks basically like going down the paths of a binary tree. And basically, I tell you that, OK, the word function this person cared about is zero everywhere, except for one of the leaf nodes in this tree. And my discriminator says, OK, I'm going to pick this one leaf node to be 1, everywhere else to be 0. Then my learner needs to explore the entire tree to figure out where the one node is that is non-zero. And that's a huge waste of time and compute and everything. The insight we had in this work was that, well, if I know the states the person actually was in, I knew the person always went to the left in this tree, went to the leftmost node. I don't actually need to look at the rest of the tree, right? Because they never went there. That's probably the wrong thing to do. So I should be focusing my optimization just on the leftmost path. If you think about it that way, I think it's pretty reasonable to say that, OK, if I really just focus on optimizing on states that were related to what I saw the expert do, I should be able to cut out a lot of the unnecessary exploration. And so did you cut out all exploration, or did you cut out unnecessary exploration, as you said, by constraining the search space of the states that you're looking at? Yeah, that's a really good question. So if I cut out all exploration, that's very close to doing something that's fully offline, right? And at that point, we wouldn't really have any robustness to compounding errors, right? Because we could just sort of end up in a place we didn't expect. But if what I do is I have enough exploration that I learned to recover my own mistakes, but not so much that I, you know, explore the entire space of the world to figure out do this one thing, I can kind of balance these two things. So I can be both computationally efficient and not sort of end up in situations I don't expect and don't know how to recover from. Sure. You kind of constrain the search space and then you use some alternate method. That's not reinforcement learning to kind of navigate through it. Yeah, so what we did was actually, I think, a very simple idea, which was that we basically did something very close to reinforcement learning, but we just changed the start state distribution to be that of the policy you want to imitate. And because of that right, I'm still doing sequential decision making, I'm still learning to recover from multi-step mistakes. But I'm saying, hey, you don't need to start at the top of this tree and figure out how to get to the bottom of the tree every single time. I'm telling you, okay, you're only going to be on the left part of the tree. Just make sure you're good there and then you're fine. So this is kind of how you can sort of constrain the search space, but still actually doing search or reinforcement learning. So I think it's really like still, it's sort of, I think, the best of both worlds in the sense that you are getting rid of the part of reinforcement learning that's challenging, which is expiration, while keeping the part of it that is helpful, which is actually learning to recover from your own mistakes. And so talk a little bit about how you kind of assessed and evaluated your results for the paper. Yeah, so we thus far only got to try out things in simulation. I'm really excited about trying this out on more real-world problems soon, but what we did was we basically picked some of these sort of Majoco sort of OpenAI gym environments, and what we tried to do was basically see, you know, how well can we sort of imitate some expert policies. So we trained some policy via enforcement learning, and we said, okay, we want to learn another policy that does the same thing. How can we do this quickly? And what we did was we picked problems that are very challenging exploration problems. So imagine something like you're controlling a four-legged creature that walks through a large maze. You don't go through every single path in this maze if you're doing it with traditional reinforcement learning. But if we basically tell you, hey, these are the set of waypoints through the maze that you only need to care about, it's a much easier optimization problem. And when we tried it out on this sort of problem, we saw rather remarkable improvements in terms of the amount of interaction with the environment needed. And there's the computational benefit of it. But I also would say that I think there's a bit of a safety benefit in the sense that if your environment is actually something that is in the real world, you don't want your learner going around and doing crazy things all the time. You want to keep them in a reasonable place. And in this example, were you worried about extracting waypoints from expert data? Or did you assume that that was a downstream or upstream task, depending on perspective? That's a good question. I guess I would say it's an upstream task, but it's one that wasn't too bad. Basically, what we could do is basically say that, OK, every time step or every few time steps, where was the expert at that time? And then we could just grab that point and say, OK, you're a waypoint. We can start the learner from this waypoint and see where they go. And so in terms of, did you have existing data sets that you were able to apply to this and benchmark results or were you, how did you benchmark your performance? Yeah. So what we did is we picked one of these sort of like standard environments that sort of, you know, a bunch of different methods have been tried on. These environments also come with a set of data. So we actually took some of the data sets from this sort of offline RL literature, and we used that set of data. And then we sort of implemented each of the methods ourselves. And you see this, you've used the autonomous vehicle analogy several times in describing this work. Do you see this method scaling up to that application? That's a far way from the four-legged open AI gym type of environment. I don't mean if you have four wheels, but I think it's actually a very reasonable application. Actually, it's one I'm pretty excited about. Another one I'm super excited about is I recently learned that a lot of the routes in Google Maps are now calculated using inverse reinforcement learning. I think it's actually one of the world's most widely deployed machine learning systems now. And I think that these sort of techniques could be really, really useful there for drastically reducing the amount of compute you need to do to compute routes and stuff like that. And I think also for self-driving, I think it's the sort of thing where even to this day, I think a good chunk of the self-driving industry, really the sort of bread and butter of the way their autonomous systems work is using different sorts of reinforcement learning or using inverse reinforcement learning techniques. So i could see this being applicable to a really wide set of kind of real world systems and it's also a set of it's also an application i think is very reasonable in the sense that one of the sort of tricky parts about our method and one thing we're trying to address in future work. is that I need to have a sort of a simulator environment where I can just put the learner in some state to start them off, right? Perhaps for like real world robots is really challenging, right? Like if I'm trying to get, you know, the robot to do a backflip, I don't know how I like reset the robot to be, you know, flipped halfway up in the air and then start it, right? But a lot of kind of stuff in the self-driving space, training is done in simulation. So this is actually a very reasonable thing like it shouldn't be hard at all to just basically you know just move the car to a different position in the simulator. So it's the sort of thing where I think actually self-driving is a sort of a perfect application of this sort of algorithm. I'm also very excited about the MacBing application because I think both of them are settings where this sort of thing should be able to help a lot. A sort of third application I'm really excited about is kind of in this space of large language models. So if you think about the sort of training of these models, right, there's a often a sort of fine tuning stuff at the end called RLHF or reinforcement learning from human feedback. And there, once again, we're using a very expensive reinforcement learning procedure. But we also have data of what we actually wanted. We know this was the data that was generated that people preferred. So it feels like we should be able to do something very similar there to basically speed up that search a lot. And there, given how compute-attentive it is to train these models, I could expect this to provide really strong computational benefits. So I guess what I'm trying to say here is that I think that there is a wide set of problems that kind of satisfy the assumptions required for this method to work out well. And I'm really excited about those applications. But I also think it's a very interesting question to think about for the problems where we can't do this really complicated, you know, agile robotics, what are sort of alternative approaches that could be useful there? Awesome. You've also got a couple of workshop papers at the conference. One is complimenting a policy with a different observation space. Can you tell us a little bit about that one and what you're trying to do there? Yeah, of course. So I think this sort of touches on what I was talking a little bit about earlier, which is, you know, trying to make decisions without, you know, access to all the observed features. So the setup for this paper, I think, was pretty interesting. So let's say we have data of some doctors, a set of doctors, interacting with some patients. So we get data of what treatment they gave and what notes they perhaps took down. And we also see whether the patient got better or worse, things like that. And then what we want to do is we want to learn some sort of decision support system, some sort of computerized agent that's able to help them. But because this agent is a computer rather than a person, they're not going to be able to see the same set of things. They're not going to get the same set of observations. And the question is, OK, how do I actually figure out how to help you? And let me try to give you a concrete example of why this is actually not an easy thing to do. So let's say I have a doctor who is prescribing chemotherapy to patients. And let's say the sort of feature they observe is some test that tells you with a high probability whether this person has cancer or not. And let's say this doctor is a really good doctor. So they basically only give you chemotherapy if you actually need it, otherwise they don't. And let's say, just for the sake of argument, our computerized agent here doesn't observe the result of this test. Well, what it's going to see is that every single time someone got chemotherapy, they got better. So it's then going to say, OK, the optimal thing for me to do is prescribe everyone chemotherapy because it only makes people get better. And then if you try to say, OK, and you try to trust the system a bit more, unless the vast majority of people had an underlying cancer condition, people are going to get worse as a result. So the question in this paper was, OK, how do we still learn how to do this well? we spend a bunch of time using some techniques from the sort of causal inference literature to try and basically kind of correctly estimate the sort of effective and intervention like giving someone a drug and try to use that to really learn these sort of decisions of systems that are able to actually help. And so is this an application of causal modeling approaches? Yeah, yeah, fully. Yeah. Okay. And so talk a little bit about the specific approach you took. Yeah, yeah. So we sort of considered a couple different settings in the paper and each required a sort of, I guess, kind of different approach or different set of assumptions. So I think maybe the easiest setting was basically, okay, at train time, but not at test time, I get to see both sets of observations. So for whatever reason, I also get to see what the doctor saw. But you know, at test time, I'm not going to get to see that. The system doesn't get to get that. You can basically use this technique called the backdoor adjustment that was pioneered by Peter Pearl back in the day. And that sort of lets you effectively use an important sampling correction to fix incorrect estimates. So basically you'd be able to sort of figure out that, hey, you know, it's this feature I didn't observe that actually was causing this positive effect, so I shouldn't like kind of over misattribute it to something else. I shouldn't overestimate the value of the chemotherapy. Of course, this is a somewhat unrealistic setting, so then we spend some time thinking about harder settings. So one setting is, okay, you don't get to see what the doctor was looking at, but I do tell you what was their probability of taking this action. So how likely were they actually to give this person chemotherapy? And then what you can do is you can again do a sort of important sampling technique to kind of fix things. The sort of hardest setting is when I don't give you either of those. I just give you the actions, and I give you a different set of observations. And if you think about this, this is a really hard problem, right? There's no reason to believe without assumptions you could actually solve this problem, right? It's like accessing your self-driving car to be able to stop, even though it doesn't see the stop sign. It's a really hard problem. So in this case, you're giving the model a set of actions that doctors took to, you know, treatment actions without showing the outcomes. So we're giving them the actions and the outcomes, but not what the doctor saw when they were choosing to give them that treatment. Ah, not the observations. Yeah. Yeah. Exactly. Exactly. Yeah. Yeah. If we don't give them the outcomes, very, yeah, really hard. Yeah. So, you know, it's like the way I like to think about this, it's almost like imagine if you and I were playing a game and it's to predict the outcome of a coin flip and I get to see the outcome of the coin flip. I'm definitely going to win this game, right? So you can't do this without any assumptions. So in particular, there's this technique from the sort of econometrics literature called a proxy correction, which simply put basically says that even if there's something I don't observe that influences a relationship I care about, So long as I actually have proxies for this, that kind of vary in interesting ways, I'm able to use these to basically kind of filter out the effect of this thing I don't observe. So we actually use a more modern version of those techniques to basically kind of correctly estimate the effect of actually giving a person a drug. I mean, it sounds like a lot of what you're trying to do here is to correct for kind of sampling imbalance in your data set? Is that a fair assessment of challenge? That's an interesting question. I guess it's, it's a sort of correcting for sampling balance. And it's a very interesting sampling balance where it's one where the sampling was done based on something you don't observe. And because of that, if you make conclusions based on this data without accounting for that fact, you're going to draw incorrect conclusions. It's definitely a sort of that sort of thing, but it's a very precise sort of sampling balance. So the generator function of the, you know, what's in your observation set and what's not is kind of out of scope until you've got this whole other set of observations that you just don't have access to and you're trying to as efficiently as possible kind of identify ways to modify the actions you would take on your observations based on some knowledge of what the rest of the world is like. Yeah, that's a great way of putting it. Yeah, exactly. And then the other workshop paper is one called Learning Shared Safety Constraints from Multitask Demonstrations. Talk a little bit about the setting there. Sure, yeah. So I think for like a variety of tasks you would want, you know, some sort of agent to do, there's kind of a background set of shared safety constraints you might care about, right? So, you know, regardless of whether somebody is cleaning the table or, you know, making a sandwich in your kitchen, they shouldn't set the kitchen on fire. That would be nice yeah yeah i think i'd be pretty mad if someone did that and i think the question is you know how we can try and learn these sorts of constraints from demonstrations and if you think about this is really similar to what i was talking about earlier we're trying to learn the words from demonstrations and i think we're trying to do is use a very sort of similar flavor of approach but here to try and learn the sort of like safety constraints. And the idea here is that it's similar in a sense to what we talked about previously. You've got a whole set of observations. The observations are all focused on kind of what to do. and you want to infer what not to do, but there's certainly a lot of things that aren't done in your observation set. That's a really interesting insight, exactly. And so I think the way we tried to frame this problem is basically this. Let's say I told you what the task the person was trying to do was. You know the reward function. And I also give you what they actually did. So any suboptimal action they took then has to be because, oh, there was a safety constraint, right? Like the reason they didn't, if they were trying to get to the destination as fast as possible, right? If they didn't run through all the other cars, it must be because they don't want to hit the other cars. So you can use this sort of comparison between the optimal behavior under the reward function and the actual behavior you saw to try to extract what explains this difference. But if you over apply that heuristic, then you also limit your ability to make the process more efficient, learn better ways to do it, things like that. Is that kind of the core? limitation that you're fighting against? Yeah, good question. I think the core limitation we are fighting against is, well, this problem is a really kind of ill-posed problem in some sense, in the sense that if for everything I don't see, I could just say there's a constraint that you can't do that. Right. But it's possible that there was just no need to do that, not that it was unsafe to do that. So the kind of key thing we were focusing on this work is saying, okay, how do we fix that problem? Yeah. And our insight was that, well, what we need is just a lot of multitask data. When you see people doing all these different things in the environment, make a sandwich, you see them clean the table, you see them wash the dishes, and in none of these things you see them set the kitchen on fire, you can probably safely assume you're not supposed to set the kitchen on fire. So it's sort of aggregating this data from a lot of different tasks, so you don't kind of learn this overly conservative constraint. In a sense to oversimplify perhaps like that seems like the obvious solution to the problem, right? If you're observing or if your models kind of taking these observations and trying to identify what's unsafe and anything it doesn't see, it's going to deem unsafe. We'll just make sure it sees a lot and a lot of different things. If we're talking about the NRL type of setting, you know, we've previously talked about how expensive that can be. Do you also look at the efficiency aspects of it? Is there something about the way that you approach multitaskness that helps to deal with that? Yeah, that's a really good question. So we didn't really explore it in much in this particular paper, but you could actually use the algorithm we talked about in the first paper for just basically resetting the states from the expert demonstrations to basically try and solve this problem faster. So you could basically take out the reinforcement learning part of this paper and just stick in the algorithm we had earlier. And I think basically everything would go through, both in theory and in practice. So that's why I think I'm particularly excited about the techniques we talked about in the first paper, because I think of it as a hammer that can be used for a really wide set of sequential decision-making problems. And so what data sets did you use for this particular paper? Yeah, so for this particular paper, we actually wanted to make the problem really hard. So we took some of the standard offline RL benchmarks and we just made them much harder. And we actually have a result that I'm really excited about in this paper, which is that We have this know this agent for legged ant running on this maze and we're able to recover all the walls of the maze the full structure of the maze without the end ever interacting with any of the walls just by looking at the maze that was really exciting to me cuz i was like. This is really good news. I've seen people try to approach this problem before, but usually they're not able to solve anything beyond a linear or tabular problem. It was just here where we were able to do something I was like, I could actually imagine using this. And I think the reason that's true for this method is it's built on this kind of strong theoretical and algorithmic foundation of techniques on the inverse reinforcement manager literature. which, you know, given they work reliably, you know, in the real world for things like self-driving cars and mapping, it's not super surprising they also worked well for this problem. So I think it's sort of by building on a rather solid piece of bedrock, I think I'm really happy that the results work as well as they did. And so besides applying that first paper that we talked about, lots of different areas, what are you most looking forward to in terms of your research agenda? That's a good question. I think I have a few different things that I'm still trying to figure out. So one of them is the sort of key assumption we're making in that first paper is that we have access to what people in the theory literature will call a generative model access to the environment. which basically means that I can just plop the learner in some random state and then see what they do from there, right? And I'm very interested in the question of, well, if we can't do that, which is true for certain problems, how do we still curtail unnecessary exploration? I think that's a very interesting theoretical question. I don't know if I have a super good answer to it yet, but I think basically you can imagine something of the form that any time you go pretty far outside of the state distribution of the expert, we just assume bad things happen. And the learner should learn, hey, I should probably not do that. So that's one thing I'm very excited about. Another thing I'm pretty excited about these days is sort of these kind of space of large language models. I think they're a really interesting kind of domain because I think the sort of Core where you kinda get them started with the supervised you know kind of training right and i just want a bunch of time arguing about basically you should care about the sort of better out with just to supervise learning. What then is right you just do supervise learning for at least getting them started and it works great and i think the sort of inside there is what we have a huge amount of data like a whole internet's worth and you have like a model with you know however many bajillion parameters then perhaps you don't need to care as much about you know the right algorithms for these problems. But as soon as you start to get to a problem where you don't have as much data, then I think you need to care more about how you do things in an efficient manner. And I think the place you see that for LLMs is in the fine-tuning steps, so specifically the RLHF step. There, because you actually need to get a person to rate which of these completions was better. You can't get an infinite amount of data of this. And at that point, you have to be a lot more careful about algorithms. So I'm really excited about trying to do that problem more efficiently. The other sort of problem there that I think is really technically interesting is people always describe RLHF as sort of an alignment procedure. And the question is like, well, alignment with who, right? It's like with this flow of raiders, all of whom might have different preferences and stuff like that. And there's a lot of kind of literature in the sort of economics and social choice theory spaces about basically how do we aggregate different preferences in a way that is reasonable. And I think there's a very interesting question there about how do we do that for reinforcement learning and giving feedback. I spent some time thinking about that. The other problem, which I, it's a little bit on the back burner. One of the sort of settings in which I think you have this kind of flavor problem I really like, which is that you have repeated interaction and you have kind of partial durability is problems in the recommendation space. So if i'm recommending you content right content i recommend you change your preference in some way right like if i tell you i kind of introduce you to a new genre of music well then i'm probably going to you know want to listen to more music like that but i don't actually ever observe your preferences right all i observe is that you clicked on this thing. So you're in the setting where it's very similar to the sort of settings I was thinking about earlier. And I think there's a really interesting space of trying to kind of adapt the algorithms I've talked about previously to these problems. And there's some preliminary work that's trying to get at this. I think there's some really beautiful work in the Spotify team on sort of treating recommendation as a sequential decision-making problem rather than just like a one-step problem. And you see like really improved benefits. Actually, I think podcast recommendations in Spotify are now done using reinforcement learning. And I think that some of the techniques I've been working on might be really useful for dealing with the fact that, hey, we don't actually observe everything that's going on here, and we need to be a little bit more careful about the way we make decisions. Nice. Awesome. Well, thanks so much for joining us to share a bit about your ICML papers and some of your research. Yeah, thank you so much for having me. Awesome. Thanks, Coco. All right, everyone, that's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit TwiMLAI.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."
    },
    "podcast_summary": "In this podcast, Gokul Swamy, a PhD student at Carnegie Mellon University, discusses his research on efficient interactive learning and decision-making without observable boundaries. He focuses on two key papers presented at the ICML conference. \n\nThe first paper introduces a method called inverse reinforcement learning without reinforcement learning. This approach efficiently learns to make sequential decisions without the need for extensive exploration. By constraining the search space and using a different start state distribution, the algorithm can optimize decisions based on observed expert behavior without repeatedly solving complex reinforcement learning problems.\n\nThe second paper explores learning shared safety constraints from multitask demonstrations. It addresses the challenge of inferring safety",
    "podcast_guest": {
        "name": "Gokul Swamy",
        "job": "PhD student at the Robotics Institute at Carnegie Mellon University",
        "wiki_title": "",
        "wiki_summary": "",
        "wiki_url": "",
        "wiki_img": "",
        "google_URL": "https://gokul.dev/"
    },
    "podcast_highlights": "Highlight 1 of the podcast: \"We're able to recover all the walls of the maze without the agent ever interacting with any of the walls just by looking at the maze.\"\n\nHighlight 2 of the podcast: \"I think self-driving is a perfect application of this algorithm.\"\n\nHighlight 3 of the podcast: \"We should be able to drastically reduce the amount of compute needed to compute routes using inverse reinforcement learning.\"\n\nHighlight 4 of the podcast: \"We want to learn a policy that doesn't observe everything that's going on and make decisions more carefully.\"\n\nHighlight 5 of the podcast: \"Learning shared safety constraints from multitask."
}